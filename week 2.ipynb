{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e0caac-d368-4383-a1c3-04c9fa3156d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence: (['-DOCSTART-'], ['O'])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "train_file = \"conll2003/eng.testb\"\n",
    "valid_file = \"conll2003/eng.testa\"\n",
    "test_file = \"conll2003/eng.train\"\n",
    "\n",
    "# Function to read CoNLL-2003 formatted files\n",
    "def read_conll_data(conll2003):\n",
    "    with open(conll2003, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences, sentence, labels = [], [], []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:  # If the line is not empty\n",
    "            parts = line.split()\n",
    "            sentence.append(parts[0])  # Token (word)\n",
    "            labels.append(parts[-1])  # NER label (last column)\n",
    "        else:  # Empty line means the end of a sentence\n",
    "            if sentence:\n",
    "                sentences.append((sentence, labels))\n",
    "                sentence, labels = [], []  # Reset for the next sentence\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Load dataset\n",
    "train_data = read_conll_data(train_file)\n",
    "valid_data = read_conll_data(valid_file)\n",
    "test_data = read_conll_data(test_file)\n",
    "\n",
    "# Print sample sentence\n",
    "print(\"Sample sentence:\", train_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170182d4-f71d-4eae-a738-e6fb59dcfac0",
   "metadata": {},
   "source": [
    "### Dataset Structure: First Sample Sentence\n",
    "\n",
    "The dataset follows the **CoNLL-2003 format**, where each sentence consists of words and their corresponding **Named Entity Recognition (NER) labels**.\n",
    "\n",
    "\n",
    "### **Explanation:**\n",
    "- The first sentence contains only the token `-DOCSTART-`, which is a **document separator marker** in the dataset.\n",
    "- The label **\"O\"** (Outside) means that this token **does not belong to any named entity**.\n",
    "- This confirms that our dataset is correctly loaded and structured.\n",
    "\n",
    "### **Key Takeaways:**\n",
    "✅ **The dataset is read correctly, and sentences are extracted.**  \n",
    "✅ **\"-DOCSTART-\" is a document boundary marker, not an actual sentence.**  \n",
    "✅ **Each token has a corresponding entity label.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c7cc10-bd70-4e07-9e9a-20ba5d784bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  \\\n",
      "0                                       [-DOCSTART-]   \n",
      "1  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
      "2                                     [Nadim, Ladki]   \n",
      "3    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
      "4  [Japan, began, the, defence, of, their, Asian,...   \n",
      "\n",
      "                                              labels  \n",
      "0                                                [O]  \n",
      "1       [O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O]  \n",
      "2                                     [B-PER, I-PER]  \n",
      "3                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]  \n",
      "4  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...  \n"
     ]
    }
   ],
   "source": [
    "# Convert dataset into a structured DataFrame\n",
    "train_sentences, train_labels = zip(*train_data)  # Unpack sentences and labels\n",
    "train_df = pd.DataFrame({\"sentence\": train_sentences, \"labels\": train_labels})\n",
    "\n",
    "# Display first few rows\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b4d2e9-5bb2-4c8f-b398-23c97245a39b",
   "metadata": {},
   "source": [
    "## Dataset Formatting: Converting Sentences to DataFrame\n",
    "\n",
    "We convert the dataset into a **structured Pandas DataFrame** for easier manipulation and analysis.\n",
    "\n",
    "### **Output:**\n",
    "| Sentence Example | Corresponding Labels |\n",
    "|-----------------|----------------------|\n",
    "| `[-DOCSTART-]` | `[O]` |\n",
    "| `[SOCCER, -, JAPAN, GET, LUCKY, WIN, ...]` | `[O, O, B-LOC, O, O, O, ...]` |\n",
    "| `[Nadim, Ladki]` | `[B-PER, I-PER]` |\n",
    "| `[AL-AIN, ,, United, Arab, Emirates, ...]` | `[B-LOC, O, B-LOC, I-LOC, ...]` |\n",
    "\n",
    "### **Explanation:**\n",
    "- **Sentences and Labels:** The dataset is structured into two columns:\n",
    "  - `sentence`: A list of tokens (words).\n",
    "  - `labels`: Corresponding **NER labels**.\n",
    "- **Entity Tagging:**\n",
    "  - `B-LOC` (Beginning of a Location) → \"JAPAN\".\n",
    "  - `B-PER` (Beginning of a Person) → \"Nadim\".\n",
    "  - `I-PER` (Inside a Person Entity) → \"Ladki\".\n",
    "- **BIO Tagging Scheme:**\n",
    "  - `B-` (Beginning of an entity).\n",
    "  - `I-` (Inside an entity).\n",
    "  - `O` (Outside, not an entity).\n",
    "\n",
    "### **Key Takeaways:**\n",
    "✅ **The dataset is correctly structured in a DataFrame.**  \n",
    "✅ **Entity labels follow the BIO tagging format.**  \n",
    "✅ **Named entities (like persons, locations) are properly labeled.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b5aa48-6e4a-434c-8341-460b3028962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Example: ['[CLS]', '-', 'D', '##OC', '##ST', '##AR', '##T', '-', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "Aligned Labels: [-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Function to tokenize and align labels\n",
    "def tokenize_and_align_labels(sentences, labels):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,  # Keeps word boundaries\n",
    "        padding=\"max_length\",  # Pads sequences to the same length\n",
    "        truncation=True,  # Truncates long sequences\n",
    "        max_length=128,  # Limit input length\n",
    "        return_tensors=\"pt\"  # Returns PyTorch tensors\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokenized words to original\n",
    "        new_labels = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "        aligned_labels.append(new_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply tokenization on training data\n",
    "tokenized_train_data = tokenize_and_align_labels(train_sentences[:5], train_labels[:5])  # Tokenize first 5 sentences\n",
    "\n",
    "# Check output\n",
    "print(\"Tokenized Example:\", tokenizer.convert_ids_to_tokens(tokenized_train_data[\"input_ids\"][0]))\n",
    "print(\"Aligned Labels:\", tokenized_train_data[\"labels\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f2f7a-454b-42ed-b566-9d7245be8f6b",
   "metadata": {},
   "source": [
    "## Tokenization with BERT\n",
    "\n",
    "BERT uses **WordPiece Tokenization**, which splits words into **subwords** and aligns entity labels accordingly.\n",
    "\n",
    "### **Tokenized Example:**\n",
    "['[CLS]', '-', 'D', '##OC', '##ST', '##AR', '##T', '-', '[SEP]', '[PAD]', '[PAD]', ...]\n",
    "\n",
    "\n",
    "### **Explanation:**\n",
    "- **[CLS]** → Special token added at the start for classification tasks.\n",
    "- **[SEP]** → Separator token marking the end of a sentence.\n",
    "- **[PAD]** → Padding tokens added to ensure all sequences have the same length.\n",
    "- **Subword Tokenization:**  \n",
    "  - `\"-DOCSTART-\"` is split into: `'-', 'D', '##OC', '##ST', '##AR', '##T', '-'`.\n",
    "  - The `##` symbol indicates that the subword is part of a previous word.\n",
    "\n",
    "### **Aligned Labels:**\n",
    "[-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100, -100, -100, ...]\n",
    "\n",
    "\n",
    "- **`-100`** → Used for special tokens like `[CLS]`, `[SEP]`, and `[PAD]`, ensuring they are ignored during training.\n",
    "- **\"O\" Labels:** These remain properly mapped to actual tokens.\n",
    "\n",
    "### **Key Takeaways:**\n",
    "✅ **BERT’s tokenizer splits words into subwords (WordPiece Tokenization).**  \n",
    "✅ **Special tokens (`[CLS]`, `[SEP]`, `[PAD]`) are handled correctly.**  \n",
    "✅ **NER labels are aligned to tokenized words, ignoring padding.**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e07a807-cbc0-4de7-bdab-05d80104d608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens  \\\n",
      "0  [[CLS], -, D, ##OC, ##ST, ##AR, ##T, -, [SEP],...   \n",
      "1  [[CLS], S, ##OC, ##CE, ##R, -, J, ##AP, ##AN, ...   \n",
      "2  [[CLS], Na, ##di, ##m, La, ##d, ##ki, [SEP], [...   \n",
      "3  [[CLS], AL, -, AI, ##N, ,, United, Arab, Emira...   \n",
      "4  [[CLS], Japan, began, the, defence, of, their,...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
      "1  [101, 156, 9244, 10954, 2069, 118, 147, 12240,...   \n",
      "2  [101, 11896, 3309, 1306, 2001, 1181, 2293, 102...   \n",
      "3  [101, 18589, 118, 19016, 2249, 117, 1244, 4699...   \n",
      "4  [101, 1999, 1310, 1103, 6465, 1104, 1147, 3141...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [-100, O, O, O, O, O, O, O, -100, -100, -100, ...  \n",
      "1  [-100, O, O, O, O, O, B-LOC, B-LOC, B-LOC, O, ...  \n",
      "2  [-100, B-PER, B-PER, B-PER, I-PER, I-PER, I-PE...  \n",
      "3  [-100, B-LOC, B-LOC, B-LOC, B-LOC, O, B-LOC, I...  \n",
      "4  [-100, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Function to convert tokenized dataset into a structured format\n",
    "def convert_to_dataframe(tokenized_data, sentences, labels):\n",
    "    data = []\n",
    "    for i in range(len(sentences)):  # Iterate over each sentence\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_data[\"input_ids\"][i])  # Convert token IDs to words\n",
    "        input_ids = tokenized_data[\"input_ids\"][i].tolist()  # Token IDs\n",
    "        attention_mask = tokenized_data[\"attention_mask\"][i].tolist()  # Attention mask\n",
    "        aligned_labels = tokenized_data[\"labels\"][i]  # Labels aligned with tokens\n",
    "        \n",
    "        # Store the processed sentence\n",
    "        data.append({\n",
    "            \"tokens\": tokens,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": aligned_labels\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Convert the training dataset to a DataFrame\n",
    "train_df = convert_to_dataframe(tokenized_train_data, train_sentences[:5], train_labels[:5])  # Storing only 5 sentences for testing\n",
    "\n",
    "# Display first few rows\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cbc41fc-b1d1-4c9a-bdc8-7b415b52fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training dataset saved as CSV!\n"
     ]
    }
   ],
   "source": [
    "train_df.to_csv(\"conll2003/tokenized_train.csv\", index=False)\n",
    "print(\"Tokenized training dataset saved as CSV!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9a9119a-5a2c-4f2d-b042-c90bf9f811ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized training dataset saved as JSON!\n"
     ]
    }
   ],
   "source": [
    "train_df.to_json(\"conll2003/tokenized_train.json\", orient=\"records\", lines=True)\n",
    "print(\"Tokenized training dataset saved as JSON!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce583298-aea1-4ff6-a71a-cb5be4816a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Tokenized Dataset (CSV):\n",
      "                                              tokens  \\\n",
      "0  ['[CLS]', '-', 'D', '##OC', '##ST', '##AR', '#...   \n",
      "1  ['[CLS]', 'S', '##OC', '##CE', '##R', '-', 'J'...   \n",
      "2  ['[CLS]', 'Na', '##di', '##m', 'La', '##d', '#...   \n",
      "3  ['[CLS]', 'AL', '-', 'AI', '##N', ',', 'United...   \n",
      "4  ['[CLS]', 'Japan', 'began', 'the', 'defence', ...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
      "1  [101, 156, 9244, 10954, 2069, 118, 147, 12240,...   \n",
      "2  [101, 11896, 3309, 1306, 2001, 1181, 2293, 102...   \n",
      "3  [101, 18589, 118, 19016, 2249, 117, 1244, 4699...   \n",
      "4  [101, 1999, 1310, 1103, 6465, 1104, 1147, 3141...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [-100, 'O', 'O', 'O', 'O', 'O', 'O', 'O', -100...  \n",
      "1  [-100, 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LO...  \n",
      "2  [-100, 'B-PER', 'B-PER', 'B-PER', 'I-PER', 'I-...  \n",
      "3  [-100, 'B-LOC', 'B-LOC', 'B-LOC', 'B-LOC', 'O'...  \n",
      "4  [-100, 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MI...  \n",
      "Loaded Tokenized Dataset (JSON):\n",
      "                                              tokens  \\\n",
      "0  [[CLS], -, D, ##OC, ##ST, ##AR, ##T, -, [SEP],...   \n",
      "1  [[CLS], S, ##OC, ##CE, ##R, -, J, ##AP, ##AN, ...   \n",
      "2  [[CLS], Na, ##di, ##m, La, ##d, ##ki, [SEP], [...   \n",
      "3  [[CLS], AL, -, AI, ##N, ,, United, Arab, Emira...   \n",
      "4  [[CLS], Japan, began, the, defence, of, their,...   \n",
      "\n",
      "                                           input_ids  \\\n",
      "0  [101, 118, 141, 9244, 9272, 12426, 1942, 118, ...   \n",
      "1  [101, 156, 9244, 10954, 2069, 118, 147, 12240,...   \n",
      "2  [101, 11896, 3309, 1306, 2001, 1181, 2293, 102...   \n",
      "3  [101, 18589, 118, 19016, 2249, 117, 1244, 4699...   \n",
      "4  [101, 1999, 1310, 1103, 6465, 1104, 1147, 3141...   \n",
      "\n",
      "                                      attention_mask  \\\n",
      "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, ...   \n",
      "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "2  [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, ...   \n",
      "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                              labels  \n",
      "0  [-100, O, O, O, O, O, O, O, -100, -100, -100, ...  \n",
      "1  [-100, O, O, O, O, O, B-LOC, B-LOC, B-LOC, O, ...  \n",
      "2  [-100, B-PER, B-PER, B-PER, I-PER, I-PER, I-PE...  \n",
      "3  [-100, B-LOC, B-LOC, B-LOC, B-LOC, O, B-LOC, I...  \n",
      "4  [-100, B-LOC, O, O, O, O, O, B-MISC, I-MISC, O...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV\n",
    "loaded_train_df = pd.read_csv(\"conll2003/tokenized_train.csv\")\n",
    "print(\"Loaded Tokenized Dataset (CSV):\")\n",
    "print(loaded_train_df.head())\n",
    "\n",
    "# Load the JSON\n",
    "loaded_json_df = pd.read_json(\"conll2003/tokenized_train.json\", orient=\"records\", lines=True)\n",
    "print(\"Loaded Tokenized Dataset (JSON):\")\n",
    "print(loaded_json_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec43fad-c855-44bc-ae60-1ee4400723c7",
   "metadata": {},
   "source": [
    "### Final Deliverables:\n",
    "\n",
    "\n",
    "✅ Tokenized and cleaned dataset saved as CSV & JSON.\n",
    "\n",
    "\n",
    "\n",
    "✅ Ready for training the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c924e-8d49-4ceb-a9c1-c999f9e6da47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
